{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建一个 Spark 会话\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to Spark\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Retail Sales\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 查看数据\n",
    "df.show()\n"
   ]
  },
  {123123qw2qw2qw2qw2qw2qqw2
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Data Segmentation\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "data_path = \"C:/Users/Dr.klika/Desktop/作业/SEM4/Big Data/SparkcGemini/Retail_sales.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# 分割数据\n",
    "chunk_size = 100.0  # 浮点数\n",
    "total_chunks = int(df.count() // chunk_size + 1)\n",
    "weights = [1.0] * total_chunks\n",
    "\n",
    "# 使用 randomSplit\n",
    "chunks = df.randomSplit(weights, seed=42)\n",
    "\n",
    "# 保存每份数据\n",
    "for i, chunk_df in enumerate(chunks):\n",
    "    output_path = f\"path/to/output/chunk_{i + 1}\"\n",
    "    chunk_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "print(\"Google Generative AI module imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# 创建 SparkSession\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Split Data by Rows\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 读取数据\n",
    "# Read data\n",
    "data_path = r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# 为每行分配一个唯一的行号（使用 Date 列排序）\n",
    "# Assign a unique row number to each row (sorted using the Date column)\n",
    "window_spec = Window.orderBy(\"Date\")  \n",
    "df_with_row_numbers = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# 每份数据的大小\n",
    "# The size of each piece of data\n",
    "chunk_size = 100\n",
    "\n",
    "# 计算总行数\n",
    "# Calculate the total number of rows\n",
    "total_rows = df_with_row_numbers.count()\n",
    "\n",
    "# Calculate the total number of copies\n",
    "total_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size != 0 else 0)\n",
    "\n",
    "# Split data and save\n",
    "for i in range(total_chunks):\n",
    "    start = i * chunk_size + 1\n",
    "    end = start + chunk_size - 1\n",
    "\n",
    "    # 按范围过滤数据 Filter data by range and save each piece of data with a file name of serial number\n",
    "    chunk_df = df_with_row_numbers.filter((df_with_row_numbers.row_number >= start) & (df_with_row_numbers.row_number <= end))\n",
    "\n",
    "    # 保存每份数据，文件名为序号\n",
    "    output_path = f\"path/to/output/chunk_{i + 1}\"\n",
    "    chunk_df.drop(\"row_number\").write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# stop\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 a^7 mod 187，输入和输出均为16进制\n",
    "a_hex = input(\"Input a (hex): \")  # 输入16进制数\n",
    "#b_hex = input(\"Input b (hex): \")  # 输入16进制数\n",
    "\n",
    "# 将16进制字符串转换为整数\n",
    "a = int(a_hex, 16)\n",
    "#b = int(b_hex, 16)\n",
    "\n",
    "# 计算 a 的 7 次方模 187\n",
    "c = pow(a, 23, 187)  # 使用 pow() 函数计算 (a^7) % 187\n",
    "\n",
    "# 输出结果为16进制\n",
    "print(f\"Result (a^7 mod 187 in hex): {hex(c)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=10, Y=5\n"
     ]
    }
   ],
   "source": [
    "X=5\n",
    "Y=10\n",
    "\n",
    "#Y=Y^X\n",
    "X=X^Y\n",
    "Y=X^Y\n",
    "X=X^Y\n",
    "\n",
    "print(\"X=%d, Y=%d\"%(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 Spark 配置信息\n",
    "print(spark.sparkContext.getConf().getAll())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# 初始化Spark会话\n",
    "spark = SparkSession.builder.appName(\"CustomerPreferencePrediction\").getOrCreate()\n",
    "\n",
    "# 读取数据\n",
    "data_path = (r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\")  # 修改为数据文件路径\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# 数据预处理：将布尔类型的Holiday Effect转换为数字类型\n",
    "df = df.withColumn(\"Holiday Effect\", col(\"Holiday Effect\").cast(\"integer\"))\n",
    "\n",
    "# 字符型特征索引化\n",
    "indexer_category = StringIndexer(inputCol=\"Product Category\", outputCol=\"Product Category Index\")\n",
    "indexer_location = StringIndexer(inputCol=\"Store Location\", outputCol=\"Store Location Index\")\n",
    "indexer_day = StringIndexer(inputCol=\"Day of the Week\", outputCol=\"Day of the Week Index\")\n",
    "\n",
    "df = indexer_category.fit(df).transform(df)\n",
    "df = indexer_location.fit(df).transform(df)\n",
    "df = indexer_day.fit(df).transform(df)\n",
    "\n",
    "# 特征构建\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Units Sold\", \"Sales Revenue (USD)\", \"Discount Percentage\", \"Marketing Spend (USD)\",\n",
    "               \"Store Location Index\", \"Product Category Index\", \"Day of the Week Index\", \"Holiday Effect\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# 拆分数据集为训练集和测试集\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# 创建随机森林分类器模型，并调整maxBins参数\n",
    "rf = RandomForestClassifier(labelCol=\"Units Sold\", featuresCol=\"features\", maxBins=243)\n",
    "\n",
    "# 训练模型\n",
    "model = rf.fit(train_df)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# 评估模型性能\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Units Sold\", rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 停止Spark会话\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Step 1: 加载数据\n",
    "df = spark.read.csv(r\"C:\\\\Users\\\\Dr.klika\\\\Desktop\\\\作业\\\\SEM4\\\\Big Data\\\\SparkcGemini\\\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: 数据预处理\n",
    "# 创建 StringIndexer 为分类变量创建索引\n",
    "indexer_location = StringIndexer(inputCol=\"Store Location\", outputCol=\"Store Location Index\")\n",
    "indexer_category = StringIndexer(inputCol=\"Product Category\", outputCol=\"Product Category Index\")\n",
    "indexer_day = StringIndexer(inputCol=\"Day of the Week\", outputCol=\"Day of the Week Index\")\n",
    "\n",
    "# 将 Holiday Effect 列转换为数值型（0 或 1）\n",
    "df = df.withColumn(\"Holiday Effect Numeric\", when(df[\"Holiday Effect\"] == True, 1).otherwise(0))\n",
    "\n",
    "# 创建 StringIndexer 为 Holiday Effect 创建索引\n",
    "indexer_holiday = StringIndexer(inputCol=\"Holiday Effect Numeric\", outputCol=\"Holiday Effect Index\")\n",
    "\n",
    "# 应用 StringIndexer\n",
    "df = indexer_location.fit(df).transform(df)\n",
    "df = indexer_category.fit(df).transform(df)\n",
    "df = indexer_day.fit(df).transform(df)\n",
    "df = indexer_holiday.fit(df).transform(df)\n",
    "\n",
    "# 查看数据\n",
    "df.show(5)\n",
    "\n",
    "# Step 3: 特征工程\n",
    "# 将数值型特征和分类变量的索引化特征组合为一个特征向量\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Units Sold\", \"Sales Revenue (USD)\", \"Discount Percentage\", \"Marketing Spend (USD)\", \n",
    "               \"Store Location Index\", \"Product Category Index\", \"Day of the Week Index\", \"Holiday Effect Index\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Step 4: 构建机器学习模型\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "\n",
    "# 设置 maxBins 为 243，确保可以处理具有多个值的特征\n",
    "rf = RandomForestClassifier(labelCol=\"Units Sold\", featuresCol=\"features\", maxBins=243)\n",
    "\n",
    "# 训练模型\n",
    "model = rf.fit(train_df)\n",
    "\n",
    "# 在测试集上进行预测\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# 评估模型\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"Units Sold\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "\n",
    "# 假设我们有客户的 ID 和预测结果\n",
    "for row in predictions.select(\"Customer ID\", \"Prediction\").collect():\n",
    "    customer_id = row[\"Customer ID\"]\n",
    "    prediction = row[\"Prediction\"]\n",
    "    \n",
    "    if prediction == 1:\n",
    "        print(f\"客户 {customer_id} 可能会购买此产品。\")\n",
    "    else:\n",
    "        print(f\"客户 {customer_id} 可能不会购买此产品。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import google.generativeai as genai\n",
    " \n",
    " \n",
    "genai.configure(api_key='qw2')  # 填入自己的api_key\n",
    " \n",
    "# 查询模型\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "response = model.generate_content(\"告诉我太阳系中最大行星的相关知识\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Customer Preference Prediction\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 去除列名中的空格\n",
    "for col_name in df.columns:\n",
    "    new_col_name = col_name.strip()  # 去除空格\n",
    "    df = df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# 只选择前10000条记录\n",
    "df_limited = df.limit(10000)\n",
    "\n",
    "# 数据预处理：转换日期格式并提取年份\n",
    "df_limited = df_limited.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "                       .withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "# 按年和其他字段选择数据\n",
    "grouped_data = df_limited.select(\"Store ID\", \"Product ID\", \"Product Category\", \"Year\", \"Units Sold\", \n",
    "                                \"Sales Revenue (USD)\", \"Discount Percentage\", \"Marketing Spend (USD)\",\n",
    "                                \"Store Location\", \"Day of the Week\", \"Holiday Effect\")\n",
    "\n",
    "# 配置 Gemini API\n",
    "genai.configure(api_key='qw2')  # 替换为你的 Gemini API Key\n",
    "\n",
    "# 用来存储生成的内容\n",
    "all_responses = []\n",
    "\n",
    "# 按数据组处理，避免collect()直接加载所有数据，使用foreach()处理每一行\n",
    "def process_row(row):\n",
    "    try:\n",
    "        # 拼接为生成内容的上下文，包含所有字段\n",
    "        context = (f\"Product ID: {row['Product ID']}, \"\n",
    "                   f\"Product Category: {row['Product Category']}, \"\n",
    "                   f\"Year: {row['Year']}, \"\n",
    "                   f\"Units Sold: {row['Units Sold']}, \"\n",
    "                   f\"Sales Revenue: ${row['Sales Revenue (USD)']:.2f}, \"\n",
    "                   f\"Discount Percentage: {row['Discount Percentage']}%, \"\n",
    "                   f\"Marketing Spend: ${row['Marketing Spend (USD)']:.2f}, \"\n",
    "                   f\"Store Location: {row['Store Location']}, \"\n",
    "                   f\"Day of the Week: {row['Day of the Week']}, \"\n",
    "                   f\"Holiday Effect: {row['Holiday Effect']}\\n\")\n",
    "\n",
    "        # 构建生成内容的提示\n",
    "        prompt = (\n",
    "            f\"You are a data analyst, Spearsland is a supermarket, Based on the following retail sales data, write a summary of customer preferences about Retail Sales Data with Seasonal Trends & Marketing. \"\n",
    "            f\"Limit 100 words: {context}\"\n",
    "        )\n",
    "\n",
    "        model = genai.GenerativeModel('gemini-pro')  # 替换为实际的 Gemini 模型 ID\n",
    "        response = model.generate_text(prompt)\n",
    "\n",
    "        # 保存每个生成结果\n",
    "        all_responses.append(response.text)\n",
    "\n",
    "        # 打印当前的结果\n",
    "        logger.info(f\"Generated Summary for Product ID {row['Product ID']} in Store {row['Store ID']}: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating text for Product ID {row['Product ID']}: {e}\")\n",
    "        all_responses.append(\"Error generating summary.\")\n",
    "\n",
    "    # 确保每次 API 调用之间有1秒的间隔\n",
    "    time.sleep(3)\n",
    "\n",
    "# 使用 foreachPartition 来避免 collect() 一次性加载全部数据\n",
    "grouped_data.rdd.foreachPartition(lambda partition: [process_row(row) for row in partition])\n",
    "\n",
    "# 将所有生成的内容汇总到一个文本文件\n",
    "with open(\"promotion_strategies.txt\", \"w\") as file:\n",
    "    for response_text in all_responses:\n",
    "        file.write(response_text + \"\\n\\n\")\n",
    "\n",
    "# 删除缓存，释放内存\n",
    "df.unpersist()\n",
    "grouped_data.unpersist()\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Split Data by Rows\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 读取数据\n",
    "data_path = r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# 为每行分配一个唯一的行号（使用 Date 列排序）\n",
    "window_spec = Window.orderBy(\"Date\")  # 替换为你的实际需求列名\n",
    "df_with_row_numbers = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# 每份数据的大小\n",
    "chunk_size = 100\n",
    "\n",
    "# 计算总行数\n",
    "total_rows = df_with_row_numbers.count()\n",
    "\n",
    "# 计算总份数\n",
    "total_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size != 0 else 0)\n",
    "\n",
    "# 分割数据并返回处理的每个数据块\n",
    "for i in range(total_chunks):\n",
    "    start = i * chunk_size + 1\n",
    "    end = start + chunk_size - 1\n",
    "\n",
    "    # 按范围过滤数据\n",
    "    chunk_df = df_with_row_numbers.filter((df_with_row_numbers.row_number >= start) & (df_with_row_numbers.row_number <= end))\n",
    "\n",
    "    # 处理每个分块的逻辑（例如：打印每个数据块，或者进行其他操作）\n",
    "    chunk_data = chunk_df.drop(\"row_number\").collect()  # 将每个分块收集为列表或其他处理\n",
    "\n",
    "    # 示例：输出当前分块的前5行数据\n",
    "    print(f\"Chunk {i + 1}:\")\n",
    "    for row in chunk_data[:5]:  # 仅打印前5行\n",
    "        print(row)\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Retail Sales\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 查看数据\n",
    "df.show()\n",
    "\n",
    "# 假设我们提取一个产品类别和对应的销售数据作为生成内容的基础\n",
    "# 选择某些字段，并将其转换为一个简单的文本形式\n",
    "sales_data = df.select(\"Product Category\", \"Sales Revenue (USD)\").collect()\n",
    "\n",
    "# 拼接成一个字符串，以便用于生成内容\n",
    "sales_info = \"\"\n",
    "for row in sales_data:\n",
    "    sales_info += f\"产品类别: {row['Product Category']}, 销售额: {row['Sales Revenue (USD)']} USD\\n\"\n",
    "\n",
    "# 使用 Google Generative AI 来生成内容\n",
    "genai.configure(api_key='qw2')  # 填入自己的 api_key\n",
    "\n",
    "# 创建模型实例\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "# 构建生成内容的提示\n",
    "prompt = f\"基于以下销售数据，生成一段描述：\\n{sales_info}\"\n",
    "\n",
    "# 生成内容\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "# 输出生成的内容\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month, dayofweek, avg, stddev, lag, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Customer Preference Prediction\").getOrCreate()\n",
    "\n",
    "# 加载数据\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 日期处理\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"Year\", year(col(\"Date\"))) \\\n",
    "       .withColumn(\"Month\", month(col(\"Date\"))) \\\n",
    "       .withColumn(\"Day of Week\", dayofweek(col(\"Date\")))\n",
    "\n",
    "# 数据清洗与特征创建\n",
    "df = df.withColumn(\"Holiday Effect\", when(col(\"Holiday Effect\") == \"true\", 1).otherwise(0)) \\\n",
    "       .withColumn(\"Discount Effect\", col(\"Discount Percentage\") * col(\"Sales Revenue (USD)\") / 100)\n",
    "\n",
    "# 增加销量变化趋势特征\n",
    "window_spec = Window.partitionBy(\"Product Category\").orderBy(\"Date\")\n",
    "df = df.withColumn(\"Units Sold Lag\", lag(\"Units Sold\", 1).over(window_spec)) \\\n",
    "       .withColumn(\"Units Sold Change\", col(\"Units Sold\") - col(\"Units Sold Lag\"))\n",
    "\n",
    "# 选择相关特征用于建模\n",
    "features = [\n",
    "    \"Discount Percentage\", \"Marketing Spend (USD)\", \"Day of Week\", \n",
    "    \"Holiday Effect\", \"Units Sold Lag\", \"Units Sold Change\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# 移除缺失值和异常值\n",
    "df = df.na.drop()\n",
    "\n",
    "# 数据集划分\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 构建随机森林模型\n",
    "rf_model = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Units Sold\", numTrees=100)\n",
    "rf_model = rf_model.fit(train_data)\n",
    "\n",
    "# 测试模型\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 评估模型\n",
    "evaluator = RegressionEvaluator(labelCol=\"Units Sold\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# 提取重要特征\n",
    "feature_importances = list(zip(features, rf_model.featureImportances))\n",
    "sorted_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "print(\"Feature Importances:\", sorted_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, month, dayofweek, avg, stddev\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Customer Preference Prediction\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 数据预处理\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"Year\", year(col(\"Date\"))) \\\n",
    "       .withColumn(\"Month\", month(col(\"Date\"))) \\\n",
    "       .withColumn(\"Day of Week\", dayofweek(col(\"Date\")))\n",
    "\n",
    "# 提取特征数据\n",
    "summary = df.groupBy(\"Product Category\", \"Year\", \"Month\") \\\n",
    "            .agg(\n",
    "                avg(\"Units Sold\").alias(\"Avg Units Sold\"),\n",
    "                avg(\"Sales Revenue (USD)\").alias(\"Avg Revenue\"),\n",
    "                avg(\"Discount Percentage\").alias(\"Avg Discount\"),\n",
    "                avg(\"Marketing Spend (USD)\").alias(\"Avg Marketing Spend\")\n",
    "            )\n",
    "\n",
    "# 收集数据，准备生成文本的输入\n",
    "sales_data = summary.collect()\n",
    "\n",
    "# 拼接为生成内容的上下文\n",
    "context = \"\"\n",
    "for row in sales_data:\n",
    "    context += (f\"Product Category: {row['Product Category']}, \"\n",
    "                f\"Year: {row['Year']}, Month: {row['Month']}, \"\n",
    "                f\"Avg Units Sold: {row['Avg Units Sold']:.2f}, \"\n",
    "                f\"Avg Revenue: ${row['Avg Revenue']:.2f}, \"\n",
    "                f\"Avg Discount: {row['Avg Discount']:.2f}%, \"\n",
    "                f\"Avg Marketing Spend: ${row['Avg Marketing Spend']:.2f}\\n\")\n",
    "\n",
    "# 配置 Gemini API\n",
    "genai.configure(api_key='qw2')  # 替换为你的 Gemini API Key\n",
    "\n",
    "# 使用 Gemini 生成内容\n",
    "prompt = (\n",
    "    f\"Based on the following retail sales data, write a summary of customer preferences, limit 100 word. \"\n",
    ")\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')  # 替换为实际的 Gemini 模型 ID\n",
    "response = model.generate_text(prompt)\n",
    "\n",
    "# 输出生成内容\n",
    "print(\"Generated Summary and Promotion Strategies:\")\n",
    "print(response.text)\n",
    "\n",
    "# 保存生成的内容到文件\n",
    "with open(\"promotion_strategies.txt\", \"w\") as file:\n",
    "    file.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Customer Preference Prediction\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 数据预处理：转换日期格式并提取年份\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "# 按年份和产品类别分组，数据\n",
    "grouped_data = df.select(\"Product Category\", \"Year\", \"Units Sold\", \"Sales Revenue (USD)\", \n",
    "                        \"Discount Percentage\", \"Marketing Spend (USD)\")\n",
    "\n",
    "# 收集数据，准备生成文本的输入\n",
    "sales_data = grouped_data.collect()\n",
    "\n",
    "# 配置 Gemini API\n",
    "genai.configure(api_key='qw2')  # 替换为你的 Gemini API Key\n",
    "\n",
    "# 用来存储生成的内容\n",
    "all_responses = []\n",
    "\n",
    "# 按年处理每个数据组，并确保中间有1秒延迟\n",
    "for row in sales_data:\n",
    "    # 拼接为生成内容的上下文\n",
    "    context = (f\"Product Category: {row['Product Category']}, \"\n",
    "               f\"Year: {row['Year']}, \"\n",
    "               f\"Units Sold: {row['Units Sold']}, \"\n",
    "               f\"Sales Revenue: ${row['Sales Revenue (USD)']:.2f}, \"\n",
    "               f\"Discount Percentage: {row['Discount Percentage']}%, \"\n",
    "               f\"Marketing Spend: ${row['Marketing Spend (USD)']:.2f}\\n\")\n",
    "\n",
    "    # 构建生成内容的提示\n",
    "    prompt = (\n",
    "        f\"You are a data analyst, Based on the following retail sales data, write a summary of customer preferences for the year {row['Year']}. \"\n",
    "        f\"Limit 100 words: {context}\"\n",
    "    )\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')  # 替换为实际的 Gemini 模型 ID\n",
    "    response = model.generate_text(prompt)\n",
    "\n",
    "    # 保存每年的生成结果\n",
    "    all_responses.append(response.text)\n",
    "    \n",
    "    # 打印当前的结果\n",
    "    print(f\"Generated Summary for {row['Year']}:\")\n",
    "    print(response.text)\n",
    "\n",
    "    # 确保每次 API 调用之间有1秒的间隔\n",
    "    time.sleep(1)\n",
    "\n",
    "# 将所有生成的内容汇总到一个文本文件\n",
    "with open(\"promotion_strategies.txt\", \"w\") as file:\n",
    "    for response_text in all_responses:\n",
    "        file.write(response_text + \"\\n\\n\")\n",
    "\n",
    "# 删除缓存，释放内存\n",
    "df.unpersist()\n",
    "grouped_data.unpersist()\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"Customer Preference Prediction\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.csv(r\"C:\\Users\\Dr.klika\\Desktop\\作业\\SEM4\\Big Data\\SparkcGemini\\Retail_sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "for col_name in df.columns:\n",
    "    new_col_name = col_name.strip()  # 去除空格\n",
    "    df = df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# 数据预处理：转换日期格式并提取年份\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "# 按年和其他字段选择数据\n",
    "grouped_data = df.select(\"Store ID\", \"Product ID\", \"Product Category\", \"Year\", \"Units Sold\", \n",
    "                        \"Sales Revenue (USD)\", \"Discount Percentage\", \"Marketing Spend (USD)\",\n",
    "                        \"Store Location\", \"Day of the Week\", \"Holiday Effect\")\n",
    "\n",
    "# 配置 Gemini API\n",
    "genai.configure(api_key='qw2')  # 替换为你的 Gemini API Key\n",
    "\n",
    "# 用来存储生成的内容\n",
    "all_responses = []\n",
    "\n",
    "# 按数据组处理，避免collect()直接加载所有数据，使用foreach()处理每一行\n",
    "def process_row(row):\n",
    "    try:\n",
    "        # 拼接为生成内容的上下文，包含所有字段\n",
    "        context = (f\"Product ID: {row['Product ID']}, \"\n",
    "                   f\"Product Category: {row['Product Category']}, \"\n",
    "                   f\"Year: {row['Year']}, \"\n",
    "                   f\"Units Sold: {row['Units Sold']}, \"\n",
    "                   f\"Sales Revenue: ${row['Sales Revenue (USD)']:.2f}, \"\n",
    "                   f\"Discount Percentage: {row['Discount Percentage']}%, \"\n",
    "                   f\"Marketing Spend: ${row['Marketing Spend (USD)']:.2f}, \"\n",
    "                   f\"Store Location: {row['Store Location']}, \"\n",
    "                   f\"Day of the Week: {row['Day of the Week']}, \"\n",
    "                   f\"Holiday Effect: {row['Holiday Effect']}\\n\")\n",
    "\n",
    "        # 构建生成内容的提示\n",
    "        prompt = (\n",
    "            f\"You are a data analyst, Spearsland is a supermarket, Based on the following retail sales data, write a summary of customer preferences about Retail Sales Data with Seasonal Trends & Marketing. \"\n",
    "            f\"Limit 100 words: {context}\"\n",
    "        )\n",
    "\n",
    "        model = genai.GenerativeModel('gemini-pro')  # 替换为实际的 Gemini 模型 ID\n",
    "        response = model.generate_text(prompt)\n",
    "\n",
    "        # 保存每个生成结果\n",
    "        all_responses.append(response.text)\n",
    "\n",
    "        # 打印当前的结果\n",
    "        logger.info(f\"Generated Summary for Product ID {row['Product ID']} in Store {row['Store ID']}: {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating text for Product ID {row['Product ID']}: {e}\")\n",
    "        all_responses.append(\"Error generating summary.\")\n",
    "\n",
    "    # 确保每次 API 调用之间有1秒的间隔\n",
    "    time.sleep(1)\n",
    "\n",
    "# 使用 foreachPartition 来避免 collect() 一次性加载全部数据\n",
    "grouped_data.rdd.foreachPartition(lambda partition: [process_row(row) for row in partition])\n",
    "\n",
    "# 将所有生成的内容汇总到一个文本文件\n",
    "with open(\"promotion_strategies.txt\", \"w\") as file:\n",
    "    for response_text in all_responses:\n",
    "        file.write(response_text + \"\\n\\n\")\n",
    "\n",
    "# 删除缓存，释放内存\n",
    "df.unpersist()\n",
    "grouped_data.unpersist()\n",
    "\n",
    "# 停止 SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, col\n",
    "import genai\n",
    "\n",
    "# 初始化 Spark 会话\n",
    "spark = SparkSession.builder.appName(\"DataProcessing\").getOrCreate()\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"your_file.csv\")\n",
    "\n",
    "# 去除列名中的空格\n",
    "for col_name in df.columns:\n",
    "    new_col_name = col_name.strip()  # 去除空格\n",
    "    df = df.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "# 转换 Date 列为 Date 类型，并提取年份\n",
    "df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"Year\", year(col(\"Date\")))\n",
    "\n",
    "# 按年和其他字段选择数据\n",
    "grouped_data = df.select(\n",
    "    \"Store ID\", \"Product ID\", \"Product Category\", \"Year\", \"Units Sold\", \n",
    "    \"Sales Revenue (USD)\", \"Discount Percentage\", \"Marketing Spend (U\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
